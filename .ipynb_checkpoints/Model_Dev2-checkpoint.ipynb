{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe30857a",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "A convolutional neural network (CNN) is implemented using Keras as an interface for TensorFlow. CNNs are artificial neural networks designed for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b58fc6",
   "metadata": {},
   "source": [
    "The genres were obtained by extracting data from the metadata of each .mp3 file using the mutagen python module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c2ae606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display\n",
    "import random\n",
    "import warnings\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf # this includes keras, keras.layers in TensorFlow 2.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8d026",
   "metadata": {},
   "source": [
    "## Melspec functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa84dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_file_to_mel_spectrogram(filepath, num_bins, hop_length):\n",
    "    \"\"\"\n",
    "    Takes in the audiofile and converts it to a Mel Spectrogram\n",
    "    :param filepath [string]:\n",
    "\n",
    "    :return mel_spect [numpy.ndarray]:\n",
    "    \"\"\"\n",
    "    # Read in audio file\n",
    "    y, sr = librosa.load(filepath, sr=None, mono=True)\n",
    "\n",
    "    # Get image window (aka image length)\n",
    "    window = create_image_window(y, hop_length)\n",
    "\n",
    "    # Get Mel Spectrogram Features\n",
    "    mel_spect = librosa.feature.melspectrogram(y=window, sr=sr,\n",
    "                                               n_fft=hop_length*2,\n",
    "                                               n_mels=num_bins,\n",
    "                                               hop_length=hop_length)\n",
    "    # Convert to Db\n",
    "    mel_spect = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "\n",
    "    return mel_spect\n",
    "\n",
    "def create_image_window(y, hop_length):\n",
    "    \"\"\"\n",
    "    Creates how wide the image is with respect to the audio clip\n",
    "    :param y:\n",
    "    :param hop_length:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    time_steps = 384  # number of time-steps. Width of image\n",
    "\n",
    "    # extract a fixed length window\n",
    "    start_sample = 0  # starting at beginning\n",
    "\n",
    "    length_samples = time_steps * hop_length\n",
    "\n",
    "    window = y[start_sample:start_sample + length_samples]\n",
    "\n",
    "    return window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44934fd",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1257e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Instrumental': 0, 'Hip-Hop': 1, 'Folk': 2, 'Pop': 3, 'Experimental': 4, 'Electronic': 5, 'International': 6, 'Rock': 7}\n",
      "Instrumental\n",
      "Hip-Hop\n",
      "Folk\n",
      "Pop\n",
      "Experimental\n",
      "Electronic\n",
      "Skipping: Electronic 099134.mp3\n",
      "International\n",
      "Rock\n",
      "Skipping: Rock 108925.mp3\n",
      "Skipping: Rock 041965.mp3\n"
     ]
    }
   ],
   "source": [
    "# Perform image augmentation to create additional spectrographs (suggest trying with and without)\n",
    "# Ref: https://www.kdnuggets.com/2020/02/audio-data-analysis-deep-learning-python-part-2.html\n",
    "# Ref: https://keras.io/api/preprocessing/image/\n",
    "# Load Libraries\n",
    "\n",
    "MP3_DIR = '/home/atj64/Downloads/fma_small/'\n",
    "IMG_DIR = '/home/atj64/Downloads/fma_melspecs/'\n",
    "wd = os.chdir(MP3_DIR)\n",
    "genres = os.listdir(wd)\n",
    "genre_dict = {}\n",
    "for genre, x in enumerate(genres):\n",
    "    genre_dict[x] = genre\n",
    "print(genre_dict)\n",
    "\n",
    "mel_specs = []\n",
    "mel_genre = []\n",
    "for genre in genres:\n",
    "    print(genre)\n",
    "    genre_dir = os.chdir(MP3_DIR + genre)\n",
    "    songs = os.listdir(wd)\n",
    "    for song in songs:\n",
    "        try:\n",
    "            mel_specs.append(audio_file_to_mel_spectrogram(MP3_DIR+genre+'/'+song,num_bins=128,hop_length=512))\n",
    "            mel_genre.append(genre_dict[genre])\n",
    "        except:\n",
    "            print(\"Skipping: \" + genre + \" \" + song)\n",
    "X = np.array(mel_specs)\n",
    "y = mel_genre\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452080a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "\n",
    "'''\n",
    "dataset = tf.keras.image_dataset_from_directory(\n",
    "    IMG_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"category\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255, # normalize the dataset\n",
    "    shear_range=0.2, # randomize some transformations\n",
    "    zoom_range=0.2, # zoom\n",
    "    horizontal_flip=True) # is this needed or helpful?\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow images from a directory\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    './training_data_fma/train',\n",
    "    target_size=(64,64), # to just use default size, this would be None\n",
    "    batch_size=47, # 32 is default, should evenly divide total number of files. 4606 files in train directory\n",
    "    class_mode='categorical', # categorical - must include y_col column with classes of each image\n",
    "    shuffle = False)\n",
    "\n",
    "val_set = val_datagen.flow_from_directory(\n",
    "    './training_data_fma/val',\n",
    "    target_size=(64,64), # must be same size as target\n",
    "    batch_size=47,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "'''\n",
    "# Create a CNN\n",
    "model = tf.keras.Sequential() # groups a linear stack of layers\n",
    "input_shape=(64,64,3) # required so model knows input shape from the start. Add to first layer.\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), strides=(2,2), input_shape=input_shape)) #produce tensor of outputs\n",
    "model.add(tf.keras.layers.AveragePooling2D((2, 2), strides=(2,2))) # average pooling for spatial data\n",
    "model.add(tf.keras.layers.Activation('relu'))#2nd hidden layer, Rectified linear unit activation function\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(tf.keras.layers.AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Activation('relu'))#3rd hidden layer\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(tf.keras.layers.AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Activation('relu'))#Flatten\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))#Add fully connected layer.\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))#Output layer\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Activation('softmax')) # activation function for output layer on multi-class classifications\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model using stochastic gradient descent\n",
    "# Ref 2: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD \n",
    "# Using default values from tutorial, except 16 epochs rather than 200 to start with\n",
    "epochs = 200\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.9\n",
    "sgd = tf.keras.optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3816332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "# time it\n",
    "model.fit_generator(\n",
    "    training_set,\n",
    "    steps_per_epoch=90,\n",
    "    epochs=50,\n",
    "    validation_data=val_set,\n",
    "    validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee55204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
