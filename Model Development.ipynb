{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe30857a",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "A convolutional neural network (CNN) is implemented using Keras as an interface for TensorFlow. CNNs are artificial neural networks designed for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b58fc6",
   "metadata": {},
   "source": [
    "The genres were obtained by extracting data from the metadata of each .mp3 file using the mutagen python module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2ae606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trackID</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000002.mp3</td>\n",
       "      <td>Food</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>29.988571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000005.mp3</td>\n",
       "      <td>This World</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>30.014694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000010.mp3</td>\n",
       "      <td>Freeway</td>\n",
       "      <td>Pop</td>\n",
       "      <td>29.988571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000140.mp3</td>\n",
       "      <td>Queen Of The Wires</td>\n",
       "      <td>Folk</td>\n",
       "      <td>29.988571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000141.mp3</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>Folk</td>\n",
       "      <td>29.988571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>154308.mp3</td>\n",
       "      <td>MIA</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>29.988571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>154309.mp3</td>\n",
       "      <td>A1 Symphony</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>29.988571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>154413.mp3</td>\n",
       "      <td>Do Easy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.014694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>154414.mp3</td>\n",
       "      <td>Dead Can Dance (uncensored)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.988571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>155066.mp3</td>\n",
       "      <td>Roy</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>29.988571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         trackID                        title       genre   duration\n",
       "0     000002.mp3                         Food     Hip-Hop  29.988571\n",
       "1     000005.mp3                   This World     Hip-Hop  30.014694\n",
       "2     000010.mp3                      Freeway         Pop  29.988571\n",
       "3     000140.mp3           Queen Of The Wires        Folk  29.988571\n",
       "4     000141.mp3                         Ohio        Folk  29.988571\n",
       "...          ...                          ...         ...        ...\n",
       "7995  154308.mp3                          MIA  Electronic  29.988571\n",
       "7996  154309.mp3                  A1 Symphony  Electronic  29.988571\n",
       "7997  154413.mp3                      Do Easy         NaN  30.014694\n",
       "7998  154414.mp3  Dead Can Dance (uncensored)         NaN  29.988571\n",
       "7999  155066.mp3                          Roy     Hip-Hop  29.988571\n",
       "\n",
       "[8000 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "csv_path = \"metadata.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44934fd",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f1257e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4606 images belonging to 8 classes.\n",
      "Found 1153 images belonging to 8 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 31, 31, 32)        896       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 93,768\n",
      "Trainable params: 93,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Perform image augmentation to create additional spectrographs (suggest trying with and without)\n",
    "# Ref: https://www.kdnuggets.com/2020/02/audio-data-analysis-deep-learning-python-part-2.html\n",
    "# Ref: https://keras.io/api/preprocessing/image/\n",
    "# Load Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display\n",
    "import random\n",
    "import warnings\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf # this includes keras, keras.layers in TensorFlow 2.0 \n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255, # normalize the dataset\n",
    "    shear_range=0.2, # randomize some transformations\n",
    "    zoom_range=0.2, # zoom\n",
    "    horizontal_flip=True) # is this needed or helpful?\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow images from a directory\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    './training_data_fma/train',\n",
    "    target_size=(64,64), # to just use default size, this would be None\n",
    "    batch_size=47, # 32 is default, should evenly divide total number of files. 4606 files in train directory\n",
    "    class_mode='categorical', # categorical - must include y_col column with classes of each image\n",
    "    shuffle = False)\n",
    "\n",
    "val_set = val_datagen.flow_from_directory(\n",
    "    './training_data_fma/val',\n",
    "    target_size=(64,64), # must be same size as target\n",
    "    batch_size=47,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "\n",
    "# Create a CNN\n",
    "model = tf.keras.Sequential() # groups a linear stack of layers\n",
    "input_shape=(64,64,3) # required so model knows input shape from the start. Add to first layer.\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), strides=(2,2), input_shape=input_shape)) #produce tensor of outputs\n",
    "model.add(tf.keras.layers.AveragePooling2D((2, 2), strides=(2,2))) # average pooling for spatial data\n",
    "model.add(tf.keras.layers.Activation('relu'))#2nd hidden layer, Rectified linear unit activation function\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(tf.keras.layers.AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Activation('relu'))#3rd hidden layer\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(tf.keras.layers.AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Activation('relu'))#Flatten\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))#Add fully connected layer.\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))#Output layer\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Activation('softmax')) # activation function for output layer on multi-class classifications\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd19989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model using stochastic gradient descent\n",
    "# Ref 2: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD \n",
    "# Using default values from tutorial, except 16 epochs rather than 200 to start with\n",
    "epochs = 200\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.9\n",
    "sgd = tf.keras.optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3816332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/stak/users/schlickr/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 2.0691 - accuracy: 0.1476WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "90/90 [==============================] - 43s 466ms/step - loss: 2.0690 - accuracy: 0.1476 - val_loss: 1.9982 - val_accuracy: 0.1735\n",
      "Epoch 2/50\n",
      "90/90 [==============================] - 32s 353ms/step - loss: 1.9843 - accuracy: 0.1915\n",
      "Epoch 3/50\n",
      "90/90 [==============================] - 32s 356ms/step - loss: 2.0117 - accuracy: 0.1807\n",
      "Epoch 4/50\n",
      "90/90 [==============================] - 32s 354ms/step - loss: 1.9254 - accuracy: 0.2102\n",
      "Epoch 5/50\n",
      "90/90 [==============================] - 32s 355ms/step - loss: 1.9753 - accuracy: 0.1723\n",
      "Epoch 6/50\n",
      "90/90 [==============================] - 32s 356ms/step - loss: 1.9550 - accuracy: 0.1611\n",
      "Epoch 7/50\n",
      "90/90 [==============================] - 32s 356ms/step - loss: 1.9508 - accuracy: 0.1566\n",
      "Epoch 8/50\n",
      "90/90 [==============================] - 32s 357ms/step - loss: 1.9671 - accuracy: 0.1773\n",
      "Epoch 9/50\n",
      "90/90 [==============================] - 32s 355ms/step - loss: 1.9441 - accuracy: 0.1604\n",
      "Epoch 10/50\n",
      "90/90 [==============================] - 32s 355ms/step - loss: 1.9642 - accuracy: 0.1806\n",
      "Epoch 11/50\n",
      "90/90 [==============================] - 32s 356ms/step - loss: 1.9458 - accuracy: 0.1659\n",
      "Epoch 12/50\n",
      "90/90 [==============================] - 32s 352ms/step - loss: 1.9251 - accuracy: 0.1679\n",
      "Epoch 13/50\n",
      "90/90 [==============================] - 32s 355ms/step - loss: 1.9358 - accuracy: 0.1827\n",
      "Epoch 14/50\n",
      "90/90 [==============================] - 32s 351ms/step - loss: 1.9376 - accuracy: 0.1633\n",
      "Epoch 15/50\n",
      "51/90 [================>.............] - ETA: 13s - loss: 1.8893 - accuracy: 0.1833"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "# time it\n",
    "model.fit_generator(\n",
    "    training_set,\n",
    "    steps_per_epoch=90,\n",
    "    epochs=50,\n",
    "    validation_data=val_set,\n",
    "    validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee55204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
